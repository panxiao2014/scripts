{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=234):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname), encoding='utf-8') as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname), encoding='utf-8') as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_texts, train_labels), (test_texts, test_labels)) = load_imdb_sentiment_analysis_dataset(\"C:\\\\03.Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "[0 1]\n",
      "[0 1]\n",
      "This is one of my favorite family movies. Loved it when I was little and it still holds up with me now that I'm older. I still laugh at all the same old jokes and might even shed a tear a times. I never have much cared for animals talking, or at least UN-animated ones but this one I'll stand up for. It's a pretty old movie but it will always hold a place in my heart.<br /><br />There aren't any other live animal movies that I can think of at the moment that I even could compare with, let alone like as much as this one. I might be giving too much praise to this movie but I don't think show. I really holds that great message that\" Home is where the heart is.\" Or at least that's the message I gained from it. Definitely recommended for a good old family movie night.\n",
      "1\n",
      "I found this on the shelf while housesitting and bored. How can people possibly give this a 10? It's not just that it's supposed to be a feel-good redemption film (I think), because it doesn't work on that level either. Weak plot, bad dialogue, terrible acting; there's just nothing there. Harvey Keitel is decent, but has nothing to work with, and Bridget Fonda and especially Johnathon Schaech are just terrible. The plot progression (especially the relationship between Byron and Ashley) makes no sense. It seems like the writers wanted the plot to go a certain way and made it, without actually writing in the necessary bits to make it flow. It's only an hour and a half, but that's 90 minutes of your life you'll never get back.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(np.unique(train_labels))\n",
    "print(np.unique(test_labels))\n",
    "print(train_texts[10])\n",
    "print(train_labels[10])\n",
    "print(train_texts[1111])\n",
    "print(train_labels[1111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Returns the median number of words per sample given corpus.\n",
    "\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()\n",
    "    \n",
    "plot_sample_length_distribution(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and normalize text, by uisng n-gram option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val, vectorizer, selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaop\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "x_trains, x_test, vectorizer, selector = ngram_vectorize(train_texts, train_labels, test_texts)\n",
    "\n",
    "trainLen = len(train_texts)\n",
    "ratio = 0.8\n",
    "\n",
    "x_train= x_trains[0: int(trainLen*ratio)]\n",
    "train_label = train_labels[0: int(trainLen*ratio)]\n",
    "x_val= x_trains[int(trainLen*ratio): trainLen]\n",
    "val_label = train_labels[int(trainLen*ratio): trainLen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect vectorized samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(20000, 20000)\n",
      "  (0, 222)\t0.040014867\n",
      "  (0, 234)\t0.0320392\n",
      "  (0, 326)\t0.058605377\n",
      "  (0, 603)\t0.05341435\n",
      "  (0, 611)\t0.04135849\n",
      "  (0, 680)\t0.040576857\n",
      "  (0, 769)\t0.04888433\n",
      "  (0, 1013)\t0.03788184\n",
      "  (0, 1275)\t0.0412489\n",
      "  (0, 1492)\t0.052573945\n",
      "  (0, 1495)\t0.041906845\n",
      "  (0, 1636)\t0.033329703\n",
      "  (0, 1647)\t0.011337811\n",
      "  (0, 1722)\t0.021325056\n",
      "  (0, 1775)\t0.013066896\n",
      "  (0, 1788)\t0.02691619\n",
      "  (0, 2259)\t0.056404907\n",
      "  (0, 2263)\t0.03441264\n",
      "  (0, 2366)\t0.0436179\n",
      "  (0, 2729)\t0.0376579\n",
      "  (0, 2741)\t0.048326466\n",
      "  (0, 2757)\t0.024164308\n",
      "  (0, 3107)\t0.027695298\n",
      "  (0, 3320)\t0.09595785\n",
      "  (0, 3452)\t0.035089932\n",
      "  :\t:\n",
      "  (0, 16141)\t0.032155022\n",
      "  (0, 16152)\t0.028531682\n",
      "  (0, 16190)\t0.028467992\n",
      "  (0, 16263)\t0.026278986\n",
      "  (0, 16447)\t0.030293142\n",
      "  (0, 16553)\t0.04528116\n",
      "  (0, 16839)\t0.054779865\n",
      "  (0, 16941)\t0.0416279\n",
      "  (0, 16989)\t0.017922325\n",
      "  (0, 17166)\t0.022838412\n",
      "  (0, 17213)\t0.025983071\n",
      "  (0, 17324)\t0.03211185\n",
      "  (0, 17553)\t0.033501253\n",
      "  (0, 17878)\t0.0401723\n",
      "  (0, 17960)\t0.035996854\n",
      "  (0, 18342)\t0.03425876\n",
      "  (0, 19025)\t0.03465925\n",
      "  (0, 19077)\t0.018168522\n",
      "  (0, 19140)\t0.030293932\n",
      "  (0, 19265)\t0.0141024515\n",
      "  (0, 19438)\t0.042818114\n",
      "  (0, 19613)\t0.034625735\n",
      "  (0, 19617)\t0.023331102\n",
      "  (0, 19938)\t0.04360201\n",
      "  (0, 19943)\t0.065347515\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape)\n",
    "#print(x_train[0])\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another preprocessing option, convert text to sequenc model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine layer type and unit number for last layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define  multi-layer perceptrons (MLPs) model for n-gram option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(2)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training process for n-gram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_model(learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=32,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_classes=2\n",
    "\n",
    "    # Create model instance.\n",
    "    model = mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=2)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_label,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_label),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "    \n",
    "    #check if model works as expected:\n",
    "    print(\"check on train data:\")\n",
    "    results = model.evaluate(x_train, train_label)\n",
    "    print(model.metrics_names)\n",
    "    print(results)\n",
    "    \n",
    "    print(\"check on val data:\")\n",
    "    results = model.evaluate(x_val, val_label)\n",
    "    print(model.metrics_names)\n",
    "    print(results)\n",
    "\n",
    "    # Save model.\n",
    "    model.save('IMDb_mlp_model.h5')\n",
    "    return model, history['val_acc'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1000\n",
      "20000/20000 - 17s - loss: 0.5626 - acc: 0.8422 - val_loss: 0.4153 - val_acc: 0.8948\n",
      "Epoch 2/1000\n",
      "20000/20000 - 16s - loss: 0.3311 - acc: 0.8986 - val_loss: 0.2754 - val_acc: 0.9120\n",
      "Epoch 3/1000\n",
      "20000/20000 - 16s - loss: 0.2348 - acc: 0.9230 - val_loss: 0.2232 - val_acc: 0.9234\n",
      "Epoch 4/1000\n",
      "20000/20000 - 16s - loss: 0.1863 - acc: 0.9379 - val_loss: 0.1981 - val_acc: 0.9304\n",
      "Epoch 5/1000\n",
      "20000/20000 - 16s - loss: 0.1574 - acc: 0.9484 - val_loss: 0.1838 - val_acc: 0.9336\n",
      "Epoch 6/1000\n",
      "20000/20000 - 16s - loss: 0.1368 - acc: 0.9551 - val_loss: 0.1746 - val_acc: 0.9350\n",
      "Epoch 7/1000\n",
      "20000/20000 - 16s - loss: 0.1178 - acc: 0.9618 - val_loss: 0.1697 - val_acc: 0.9374\n",
      "Epoch 8/1000\n",
      "20000/20000 - 16s - loss: 0.1028 - acc: 0.9685 - val_loss: 0.1650 - val_acc: 0.9356\n",
      "Epoch 9/1000\n",
      "20000/20000 - 16s - loss: 0.0918 - acc: 0.9722 - val_loss: 0.1628 - val_acc: 0.9362\n",
      "Epoch 10/1000\n",
      "20000/20000 - 16s - loss: 0.0803 - acc: 0.9759 - val_loss: 0.1620 - val_acc: 0.9372\n",
      "Epoch 11/1000\n",
      "20000/20000 - 16s - loss: 0.0727 - acc: 0.9793 - val_loss: 0.1611 - val_acc: 0.9358\n",
      "Epoch 12/1000\n",
      "20000/20000 - 16s - loss: 0.0659 - acc: 0.9816 - val_loss: 0.1616 - val_acc: 0.9350\n",
      "Epoch 13/1000\n",
      "20000/20000 - 16s - loss: 0.0597 - acc: 0.9837 - val_loss: 0.1621 - val_acc: 0.9374\n",
      "Validation accuracy: 0.9373999834060669, loss: 0.1621284546494484\n",
      "check on train data:\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "20000/20000 [==============================] - 4s 192us/sample - loss: 0.0358 - acc: 0.9955\n",
      "['loss', 'acc']\n",
      "[0.035846447660773995, 0.9955]\n",
      "check on val data:\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "5000/5000 [==============================] - 1s 228us/sample - loss: 0.1621 - acc: 0.9374\n",
      "['loss', 'acc']\n",
      "[0.1621284542441368, 0.9374]\n"
     ]
    }
   ],
   "source": [
    "imdbModel, _, _ = train_ngram_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "4000/4000 [==============================] - 1s 185us/sample - loss: 0.2604 - acc: 0.9005\n",
      "['loss', 'acc']\n",
      "[0.2603893958628178, 0.9005]\n"
     ]
    }
   ],
   "source": [
    "results = imdbModel.evaluate(x_test, test_labels)\n",
    "print(imdbModel.metrics_names)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get test data, predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testStrings = []\n",
    "testLabels = []\n",
    "    \n",
    "for category in ['pos', 'neg']:\n",
    "    test_path = os.path.join(\"C:\\\\03.Data\\\\aclImdb\", 'test', category)\n",
    "    for fname in sorted(os.listdir(test_path)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(test_path, fname), encoding='utf-8') as f:\n",
    "                testStrings.append(f.read())\n",
    "            testLabels.append(0 if category == 'neg' else 1)\n",
    "            \n",
    "seedTest = 987\n",
    "random.seed(seedTest)\n",
    "random.shuffle(testStrings)\n",
    "random.seed(seedTest)\n",
    "random.shuffle(testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shuffle, and check some original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people, get a clue! I mean, the writers dont have one, so I would expect you to...this show is SUCH a pale imitation of the '60 show that you can laugh at it! and the sixties show is the one with the cheap special effects, and is in B&W fer Chrissakes! Yet the mood and the writing on the old show is MILES ahead of this drivel. Get HiP kids! if 98 Degrees or Brittany told you to watch it, you would! You know it! Just bypass them and tell all your friends you were 'IN' with the 'scene' BEFORE it became too cool!\n",
      "0\n",
      "I loved it. In fact, I watched it over and over and over, and I could watch it again. This movie doesn't get boring.<br /><br />The vampire concept is revolutionized in this movie. It's a job well done, great for today's generation.<br /><br />Wesley Snipes was born for this role. Stephen Dorff was an ideal vampire. Arly Jover, mmmm mmm, she can bite me any time she wants and what a sexy accent she has. Donal Logue provides great comical relief.<br /><br />This vampire movie is like no other. I can't wait for Blade 2.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(testStrings[114])\n",
    "print(testLabels[114])\n",
    "print(testStrings[786])\n",
    "print(testLabels[786])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "This film is so bad. I mean, who commissions this stuff? And the costume designer deserves an award for making everyone look like they had just stepped out of 1983. A bloke puts a female wig on and fights....nuff said.\n",
      "real value: 0\n",
      "[[0.01965372]]\n"
     ]
    }
   ],
   "source": [
    "x_index = 1901\n",
    "\n",
    "x_test = vectorizer.transform(testStrings[x_index: x_index+1])\n",
    "x_test = selector.transform(x_test).astype('float32')\n",
    "\n",
    "x_predict = imdbModel.predict(x_test)\n",
    "\n",
    "print(testStrings[x_index])\n",
    "print(\"real value: %d\" % testLabels[x_index])\n",
    "print(x_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# real comment from internet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n",
      "I am very, very tied on this film. On one hand, I thought Bohemian Rhapsody tributed the story of Freddie Mercury-for the most part-judiciously, mainly due to Rami Malek's immersive performance. Trust me, if Malek wasn't in this movie, Bohemian Rhapsody would've crumbled to ashes apace with some of the biggest catastrophes of 2018. As a depiction and honory of a musical legend, I was earnestly contented to witness the results. On the other hand, I found this movie to be a pretty incoherent, potboiler mess of a product. The harshest way to submit this is, well, simply put, to say that Bohemian Rhapsody is fundamentally your average company manufactured, cop-out, strand of merchandise that was only fabricated for mainstream audiences who will unknowingly be able to eat up the same old Fast and the Furious: Family is Important blueprint that they've unconsciously seen billions of times. However, this movie is dang LUCKY that they got Malek on the project and that they AT LEAST managed to make a glaringly enthralling, two-hour, sound-explosion of a music video..\n",
      "[[0.58261466]]\n"
     ]
    }
   ],
   "source": [
    "s = \"I am very, very tied on this film. On one hand, I thought Bohemian Rhapsody tributed the story of Freddie Mercury-for the most part-judiciously, mainly due to Rami Malek's immersive performance. Trust me, if Malek wasn't in this movie, Bohemian Rhapsody would've crumbled to ashes apace with some of the biggest catastrophes of 2018. As a depiction and honory of a musical legend, I was earnestly contented to witness the results. On the other hand, I found this movie to be a pretty incoherent, potboiler mess of a product. The harshest way to submit this is, well, simply put, to say that Bohemian Rhapsody is fundamentally your average company manufactured, cop-out, strand of merchandise that was only fabricated for mainstream audiences who will unknowingly be able to eat up the same old Fast and the Furious: Family is Important blueprint that they've unconsciously seen billions of times. However, this movie is dang LUCKY that they got Malek on the project and that they AT LEAST managed to make a glaringly enthralling, two-hour, sound-explosion of a music video..\"\n",
    "\n",
    "sList = [s]\n",
    "x_real = vectorizer.transform(sList)\n",
    "x_real = selector.transform(x_real).astype('float32')\n",
    "\n",
    "x_predict = imdbModel.predict(x_real)\n",
    "\n",
    "print(s)\n",
    "print(x_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
